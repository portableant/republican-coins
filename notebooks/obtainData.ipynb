{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc7c63d",
   "metadata": {},
   "source": [
    "# Obtaining Coin Data and Images from the Portable Antiquities Scheme Database\n",
    "\n",
    "This notebook demonstrates how to obtain coin data and images from the Portable Antiquities Scheme (PAS) database using web scraping techniques due to the imposition of restrictions on machine queries of the API that I built between 2006 and 2015. The Scheme/British Museum now uses Cloudflare to protect its API endpoints, making it difficult to access the data programmatically as a javascript challenge is thrown up. This can be bypassed and here's how. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236b862",
   "metadata": {},
   "source": [
    "## Using Python to download data and images\n",
    "\n",
    "To do this I used the python library cloudscraper in a virtual environment and created a script to handle the scraping and downloading of json to CSV and subsequently images. First off, set up your virtual environment:\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "You're now ready to go!\n",
    "\n",
    "## Code\n",
    "\n",
    "For this example, I am going to make a slight change to the actual script I ran to make this easier to manage in the notebook environment. \n",
    "Instead of all records attached to Reece Period 1, I am just taking the 48 publicly available records for coin hoards.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc72a0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a scraper instance\n",
    "# This handles the Cloudflare challenges automatically\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Define the base URL\n",
    "url_base = 'https://finds.org.uk/database/search/results/broadperiod/ROMAN/reeceID/1/objectType/HOARDformat/json'\n",
    "\n",
    "# Set a user-agent to mimic a real browser\n",
    "# Cloudscraper will add other necessary headers automatically\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the initial request and get the total number of pages\n",
    "print(\"Fetching metadata from the first page...\")\n",
    "response = scraper.get(url_base, headers=headers)\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "total_results = json_data['meta']['totalResults']\n",
    "results_per_page = json_data['meta']['resultsPerPage']\n",
    "pagination = (total_results + results_per_page - 1) // results_per_page\n",
    "\n",
    "print(f\"Total records: {total_results}\")\n",
    "print(f\"Total pages to scrape: {pagination}\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Process the first page\n",
    "records = json_data['results']\n",
    "df = pd.DataFrame(records)\n",
    "all_data.append(df)\n",
    "\n",
    "# Loop through the remaining pages\n",
    "for i in range(2, pagination + 1):\n",
    "    url_download = f\"{url_base}/page/{i}\"\n",
    "    print(f\"Scraping page {i}/{pagination}...\")\n",
    "    \n",
    "    try:\n",
    "        response_paged = scraper.get(url_download, headers=headers)\n",
    "        paged_json = json.loads(response_paged.text)\n",
    "        records_paged = paged_json['results']\n",
    "        df_paged = pd.DataFrame(records_paged)\n",
    "        all_data.append(df_paged)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred on page {i}: {e}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Concatenate all dataframes and save to CSV\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.to_csv('./data/reece1.csv', index=False, na_rep='')\n",
    "\n",
    "print(\"Data successfully scraped and saved to ./data/reece1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af45b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
