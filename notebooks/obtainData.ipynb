{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc7c63d",
   "metadata": {},
   "source": [
    "# Obtaining Coin Data and Images from the Portable Antiquities Scheme Database\n",
    "\n",
    "This notebook demonstrates how to obtain coin data and images from the Portable Antiquities Scheme (PAS) database using web scraping techniques due to the imposition of restrictions on machine queries of the API that I built between 2006 and 2015. The Scheme/British Museum now uses Cloudflare to protect its API endpoints, making it difficult to access the data programmatically as a javascript challenge is thrown up. This can be bypassed and here's how. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236b862",
   "metadata": {},
   "source": [
    "## Using Python to download data and images\n",
    "\n",
    "To do this I used the python library cloudscraper in a virtual environment and created a script to handle the scraping and downloading of json to CSV and subsequently images. First off, set up your virtual environment:\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "You're now ready to go!\n",
    "\n",
    "## Code\n",
    "\n",
    "For this example, I am going to make a slight change to the actual script I ran to make this easier to manage in the notebook environment. \n",
    "Instead of all records attached to Reece Period 1, I am going to mock the pagination object to only have 4 pages by changing the `pagination` variable to 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0ada90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cloudscraper in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (1.2.71)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from cloudscraper) (3.2.3)\n",
      "Requirement already satisfied: requests>=2.9.2 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from cloudscraper) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from requests>=2.9.2->cloudscraper) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from requests>=2.9.2->cloudscraper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from requests>=2.9.2->cloudscraper) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428643fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc72a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metadata from the first page...\n",
      "Total records: 3305\n",
      "Total pages to scrape: 4\n",
      "Scraping page 2/4...\n",
      "Scraping page 3/4...\n",
      "Scraping page 4/4...\n",
      "Data successfully scraped and saved to ./data/reece1.csv\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a scraper instance\n",
    "# This handles the Cloudflare challenges automatically\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Define the base URL\n",
    "url_base = 'https://finds.org.uk/database/search/results/broadperiod/ROMAN/reeceID/1/format/json'\n",
    "\n",
    "# Set a user-agent to mimic a real browser\n",
    "# Cloudscraper will add other necessary headers automatically\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the initial request and get the total number of pages\n",
    "print(\"Fetching metadata from the first page...\")\n",
    "response = scraper.get(url_base, headers=headers)\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "total_results = json_data['meta']['totalResults']\n",
    "results_per_page = json_data['meta']['resultsPerPage']\n",
    "# pagination = (total_results + results_per_page - 1) // results_per_page\n",
    "pagination = 4\n",
    "print(f\"Total records: {total_results}\")\n",
    "print(f\"Total pages to scrape: {pagination}\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Process the first page\n",
    "records = json_data['results']\n",
    "df = pd.DataFrame(records)\n",
    "all_data.append(df)\n",
    "\n",
    "# Loop through the remaining pages\n",
    "for i in range(2, pagination + 1):\n",
    "    url_download = f\"{url_base}/page/{i}\"\n",
    "    print(f\"Scraping page {i}/{pagination}...\")\n",
    "    \n",
    "    try:\n",
    "        response_paged = scraper.get(url_download, headers=headers)\n",
    "        paged_json = json.loads(response_paged.text)\n",
    "        records_paged = paged_json['results']\n",
    "        df_paged = pd.DataFrame(records_paged)\n",
    "        all_data.append(df_paged)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred on page {i}: {e}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Concatenate all dataframes and save to CSV\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.to_csv('./data/reece1.csv', index=False, na_rep='')\n",
    "\n",
    "print(\"Data successfully scraped and saved to ./data/reece1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5c521",
   "metadata": {},
   "source": [
    "Let's see what we got back from the JSON API, converting it to a DataFrame and csv. This can be seen by running the next command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d0645f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findIdentifier</th>\n",
       "      <th>id</th>\n",
       "      <th>old_findID</th>\n",
       "      <th>objecttype</th>\n",
       "      <th>broadperiod</th>\n",
       "      <th>description</th>\n",
       "      <th>notes</th>\n",
       "      <th>periodFrom</th>\n",
       "      <th>periodTo</th>\n",
       "      <th>fromdate</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>currentLocation</th>\n",
       "      <th>treasure</th>\n",
       "      <th>rally</th>\n",
       "      <th>TID</th>\n",
       "      <th>note</th>\n",
       "      <th>fromsubperiod</th>\n",
       "      <th>tosubperiod</th>\n",
       "      <th>subperiodFrom</th>\n",
       "      <th>subperiodTo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finds-1233746</td>\n",
       "      <td>1233746</td>\n",
       "      <td>GAT-06A665</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A silver Roman denarius of Augustus (27&amp;nbsp;B...</td>\n",
       "      <td>Until 28/08/2025 this find was grouped under I...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finds-1230870</td>\n",
       "      <td>1230870</td>\n",
       "      <td>YORYM-493963</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A silver Roman denarius of Tiberius (AD 14-37)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finds-1228996</td>\n",
       "      <td>1228996</td>\n",
       "      <td>SUR-609809</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A&amp;nbsp;worn silver Roman Republican denarius i...</td>\n",
       "      <td>Recorded from details emailed by the finder.</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finds-1227572</td>\n",
       "      <td>1227572</td>\n",
       "      <td>NMS-78A76D</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A Roman&amp;nbsp;silver&amp;nbsp;Republican denarius o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>finds-1226906</td>\n",
       "      <td>1226906</td>\n",
       "      <td>SUR-4C2E85</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>An extremely worn silver Roman Republican Dena...</td>\n",
       "      <td>Recorded from details emailed by the finder.</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finds-1226546</td>\n",
       "      <td>1226546</td>\n",
       "      <td>NMS-E7CC18</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>Copper alloy as or dupondius of uncertain Juli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finds-1225966</td>\n",
       "      <td>1225966</td>\n",
       "      <td>BERK-8F4653</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A worn silver Roman Republican denarius of Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>finds-1225964</td>\n",
       "      <td>1225964</td>\n",
       "      <td>BERK-8F1AC6</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A silver Roman Republican denarius of&amp;nbsp;P. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>finds-1225829</td>\n",
       "      <td>1225829</td>\n",
       "      <td>SF-683A10</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A complete silver Roman denarius issued by the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>finds-1225730</td>\n",
       "      <td>1225730</td>\n",
       "      <td>KENT-55AA42</td>\n",
       "      <td>COIN</td>\n",
       "      <td>ROMAN</td>\n",
       "      <td>A silver Roman republican denarius of L. Treba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  findIdentifier       id    old_findID objecttype broadperiod  \\\n",
       "0  finds-1233746  1233746    GAT-06A665       COIN       ROMAN   \n",
       "1  finds-1230870  1230870  YORYM-493963       COIN       ROMAN   \n",
       "2  finds-1228996  1228996    SUR-609809       COIN       ROMAN   \n",
       "3  finds-1227572  1227572    NMS-78A76D       COIN       ROMAN   \n",
       "4  finds-1226906  1226906    SUR-4C2E85       COIN       ROMAN   \n",
       "5  finds-1226546  1226546    NMS-E7CC18       COIN       ROMAN   \n",
       "6  finds-1225966  1225966   BERK-8F4653       COIN       ROMAN   \n",
       "7  finds-1225964  1225964   BERK-8F1AC6       COIN       ROMAN   \n",
       "8  finds-1225829  1225829     SF-683A10       COIN       ROMAN   \n",
       "9  finds-1225730  1225730   KENT-55AA42       COIN       ROMAN   \n",
       "\n",
       "                                         description  \\\n",
       "0  A silver Roman denarius of Augustus (27&nbsp;B...   \n",
       "1  A silver Roman denarius of Tiberius (AD 14-37)...   \n",
       "2  A&nbsp;worn silver Roman Republican denarius i...   \n",
       "3  A Roman&nbsp;silver&nbsp;Republican denarius o...   \n",
       "4  An extremely worn silver Roman Republican Dena...   \n",
       "5  Copper alloy as or dupondius of uncertain Juli...   \n",
       "6  A worn silver Roman Republican denarius of Mar...   \n",
       "7  A silver Roman Republican denarius of&nbsp;P. ...   \n",
       "8  A complete silver Roman denarius issued by the...   \n",
       "9  A silver Roman republican denarius of L. Treba...   \n",
       "\n",
       "                                               notes  periodFrom  periodTo  \\\n",
       "0  Until 28/08/2025 this find was grouped under I...          21        21   \n",
       "1                                                NaN          21        21   \n",
       "2       Recorded from details emailed by the finder.          21        21   \n",
       "3                                                NaN          21        21   \n",
       "4       Recorded from details emailed by the finder.          21        21   \n",
       "5                                                NaN          21        21   \n",
       "6                                                NaN          21        21   \n",
       "7                                                NaN          21        21   \n",
       "8                                                NaN          21        21   \n",
       "9                                                NaN          21        21   \n",
       "\n",
       "   fromdate  ...  length  currentLocation treasure  rally  TID note  \\\n",
       "0     -27.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "1      36.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "2     -48.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "3      32.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "4    -108.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "5      37.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "6     -32.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "7     -42.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "8     -17.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "9    -135.0  ...     NaN              NaN      NaN    NaN  NaN  NaN   \n",
       "\n",
       "  fromsubperiod  tosubperiod subperiodFrom  subperiodTo  \n",
       "0           NaN          NaN           NaN          NaN  \n",
       "1           NaN          NaN           NaN          NaN  \n",
       "2           NaN          NaN           NaN          NaN  \n",
       "3           NaN          NaN           NaN          NaN  \n",
       "4           NaN          NaN           NaN          NaN  \n",
       "5           NaN          NaN           NaN          NaN  \n",
       "6           NaN          NaN           NaN          NaN  \n",
       "7           NaN          NaN           NaN          NaN  \n",
       "8           NaN          NaN           NaN          NaN  \n",
       "9           NaN          NaN           NaN          NaN  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./data/reece1.csv').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc554f26",
   "metadata": {},
   "source": [
    "We now want to extract and save images from the database and these can be generated from several fields that exist in the csv data frame. These are filename, imagedir and a base URL of 'https://finds.org.uk/'. The next script will get the images from the database, again using cloudscraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f43b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 records from reece1.csv.\n",
      "Found 78 unique images to download.\n",
      "Skipping: GAT-06A665_68b06c8915771.jpg (already exists)\n",
      "Skipping: YORYM-493963_68a59124800a5.jpg (already exists)\n",
      "Skipping: SUR-609809_688773ef6844e.jpg (already exists)\n",
      "Skipping: SUR-4C2E85_6874db2471a8b.jpg (already exists)\n",
      "Skipping: BERK-8F4653_687611bc28843.jpg (already exists)\n",
      "Skipping: BERK-8F1AC6_687612a110ec4.jpg (already exists)\n",
      "Skipping: SF-683A10_68762c8c57dc5.jpg (already exists)\n",
      "Skipping: KENT-55AA42_68655af514f86.jpg (already exists)\n",
      "Skipping: WAW-932FCA_685a792a0d8a2.jpg (already exists)\n",
      "Skipping: SWYOR-91E15B_6862c07573466.jpg (already exists)\n",
      "Skipping: HAMP-905009_68590bb613a11.jpg (already exists)\n",
      "Skipping: LIN-561371_68a6c5f4ac9a4.jpg (already exists)\n",
      "Skipping: BUC-134FE1_68513593a9599.jpg (already exists)\n",
      "Skipping: WMID-C303A6_686d16ce9fc4d.jpg (already exists)\n",
      "Skipping: LVPL-167290_685ec780e1c94.jpg (already exists)\n",
      "Skipping: ESS-F092BB_6842e05912ec8.jpg (already exists)\n",
      "Skipping: SF-5C3305_68664e4d52a71.jpg (already exists)\n",
      "Skipping: OXON-DB458E_682db7cac04ab.jpg (already exists)\n",
      "Skipping: SF-B16CF6_68652e7cada09.jpg (already exists)\n",
      "Skipping: YORYM-5DC6ED_68501ee5d96eb.jpg (already exists)\n",
      "Skipping: LEIC-4E87FF_6824e8b8ea619.jpg (already exists)\n",
      "Skipping: WAW-34F1AE_68239793630e5.jpg (already exists)\n",
      "Skipping: SUR-31162B_682357991bda0.jpg (already exists)\n",
      "Skipping: OXON-B1F617_681b278a218b6.jpg (already exists)\n",
      "Skipping: OXON-208026_6812096783214.jpg (already exists)\n",
      "Skipping: WMID-09A2CE_6819dbda61c89.jpg (already exists)\n",
      "Skipping: YORYM-0D8600_6800dc2d7ddcc.jpg (already exists)\n",
      "Skipping: YORYM-E58340_68133f0a9754d.jpg (already exists)\n",
      "Skipping: BUC-D0BC14_67fd0c3cf3b68.jpg (already exists)\n",
      "Skipping: ESS-511D9B_6808afa78fbc2.jpg (already exists)\n",
      "Skipping: WMID-3AE27B_67f3b24532d6b.jpg (already exists)\n",
      "Skipping: GLO-FD8072_6838808c34eb6.jpg (already exists)\n",
      "Skipping: SUSS-AA2E32_67ed797c5c3de.jpg (already exists)\n",
      "Skipping: SUR-69D94C_67e6a48c4b5e5.jpg (already exists)\n",
      "Skipping: SWYOR-425A1E_6810b382a7f02.jpg (already exists)\n",
      "Skipping: ESS-407D4A_67e5327a1e8e4.jpg (already exists)\n",
      "Skipping: WMID-3E72F3_67ee654d9a97f.jpg (already exists)\n",
      "Skipping: OXON-3D0903_67e85c784f1e2.jpg (already exists)\n",
      "Skipping: SF-27EB9B_685034acf0128.jpg (already exists)\n",
      "Skipping: SF-D4E9BC_67e3f56b5fea7.jpg (already exists)\n",
      "Skipping: SF-C00464_686fa5b01dd78.jpg (already exists)\n",
      "Skipping: SUR-ADE232_67dade99159e6.jpg (already exists)\n",
      "Skipping: LVPL-AB479B_680b83e81ba0f.jpg (already exists)\n",
      "Skipping: SWYOR-804741_67f8195d8476e.jpg (already exists)\n",
      "Skipping: SUR-131C30_67d132bf08516.jpg (already exists)\n",
      "Skipping: SUR-E9366E_67ce937b6b569.jpg (already exists)\n",
      "Skipping: WREX-88411A_67c8865c65ca8.jpg (already exists)\n",
      "Skipping: WILT-706C7C_67c9b7d769986.jpg (already exists)\n",
      "Skipping: BUC-6E2C1C_67c713b28f8e7.jpg (already exists)\n",
      "Skipping: WREX-58B050_67c58c628913d.jpg (already exists)\n",
      "Skipping: DENO-03EA0C_687cfb5b303d4.jpg (already exists)\n",
      "Skipping: WREX-C9E614_67bc9efb2b4ae.jpg (already exists)\n",
      "Skipping: IOW-C4A261_67bc6aaac34d7.jpg (already exists)\n",
      "Skipping: DENO-72FB23_687cfdc91d174.jpg (already exists)\n",
      "Skipping: SF-3505CE_6853c190126a2.jpg (already exists)\n",
      "Skipping: ESS-34658D_67b3478a2d5fe.jpg (already exists)\n",
      "Skipping: SF-3016DF_6853c3a9da387.jpg (already exists)\n",
      "Skipping: BERK-0D63F7_67b1f92ac8f97.jpg (already exists)\n",
      "Skipping: BERK-0D5C1B_67b1f8b69a389.jpg (already exists)\n",
      "Skipping: BERK-DF9682_67d410eb867a0.jpg (already exists)\n",
      "Skipping: BERK-DF7E0C_67d406f5c423c.jpg (already exists)\n",
      "Skipping: BERK-DF77F7_67d402122a7df.jpg (already exists)\n",
      "Skipping: SOM-DD47B2_67bd978155775.jpg (already exists)\n",
      "Skipping: SUSS-CA1B88_67aca1d89c7aa.jpg (already exists)\n",
      "Skipping: HAMP-B3BE56_67ab8670dfebc.jpg (already exists)\n",
      "Skipping: SF-B2A999_6825f6f19caa2.jpg (already exists)\n",
      "Skipping: SUSS-A09294_67ab6a9da023e.jpg (already exists)\n",
      "Skipping: SUR-5ECF7E_67a62003bc046.jpg (already exists)\n",
      "Skipping: ESS-4D0B71_67a4d23fee25c.jpg (already exists)\n",
      "Skipping: BERK-38C727_67b1f99f6ddff.jpg (already exists)\n",
      "Skipping: WREX-37BFE7_67a37ea1de9b0.jpg (already exists)\n",
      "Skipping: OXON-347195_67ac7afbd0b97.jpg (already exists)\n",
      "Skipping: SWYOR-0BFE82_67b4ca85825c8.jpg (already exists)\n",
      "Skipping: NARC-B4DFE0_679b4e55970df.jpg (already exists)\n",
      "Skipping: SUR-8C2F5D_6799e8b326ac5.jpg (already exists)\n",
      "Skipping: YORYM-FD95AF_678640a7a84d7.jpg (already exists)\n",
      "Skipping: SF-3F0350_67924ade8c170.jpg (already exists)\n",
      "Skipping: SUSS-1967B5_6762be71a5cf4.jpg (already exists)\n",
      "\n",
      "No 404 errors were found during the download process.\n",
      "\n",
      "Download process complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from urllib3.exceptions import NotOpenSSLWarning\n",
    "\n",
    "warnings.simplefilter('ignore', NotOpenSSLWarning)\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Create a scraper instance to handle Cloudflare\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Define the base URL and the local directory for images\n",
    "base_url = 'https://finds.org.uk/'\n",
    "output_dir = './data/downloaded_images'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('./data/reece1.csv')\n",
    "    print(f\"Loaded {len(df)} records from reece1.csv.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'reece1.csv' was not found. Please ensure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize an empty DataFrame to store 404 errors\n",
    "error_log_df = pd.DataFrame(columns=['old_findID', 'imagedir', 'filename', 'error_message'])\n",
    "\n",
    "# Get the list of all unique image directories and filenames\n",
    "images_to_download = df[['old_findID', 'imagedir', 'filename']].dropna().drop_duplicates()\n",
    "\n",
    "print(f\"Found {len(images_to_download)} unique images to download.\")\n",
    "\n",
    "# Loop through the unique image paths and download each file\n",
    "for index, row in images_to_download.iterrows():\n",
    "    old_findID = row['old_findID']\n",
    "    imagedir = row['imagedir']\n",
    "    filename = row['filename']\n",
    "    \n",
    "    # Construct the full image URL\n",
    "    full_url = os.path.join(base_url, imagedir, filename).replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # Define the local path to save the image\n",
    "    local_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Skip if the file already exists\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"Skipping: {filename} (already exists)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading: {filename}\")\n",
    "    try:\n",
    "        # Get the image content\n",
    "        response = scraper.get(full_url, stream=True)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        \n",
    "        # Save the image to the local file\n",
    "        with open(local_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename}.\")\n",
    "        \n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            new_row = pd.DataFrame([{'old_findID': old_findID, 'imagedir': imagedir, 'filename': filename, 'error_message': '404 - Not Found'}])\n",
    "            error_log_df = pd.concat([error_log_df, new_row], ignore_index=True)\n",
    "            print(f\"Failed to download {filename} (404 Not Found). Logged to CSV.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename} from {full_url}. Reason: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with {filename}: {e}\")\n",
    "        \n",
    "    # Be a polite scraper and add a short delay between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the 404 error log to a CSV file\n",
    "if not error_log_df.empty:\n",
    "    error_log_df.to_csv('./data/404_errors.csv', index=False)\n",
    "    print(\"\\n404 errors have been logged to './data/404_errors.csv'.\")\n",
    "else:\n",
    "    print(\"\\nNo 404 errors were found during the download process.\")\n",
    "\n",
    "print(\"\\nDownload process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f2ebc",
   "metadata": {},
   "source": [
    "Some of the records are missing geographical coordinates due to the limitations of the original data source. As a result, we need to handle these missing values appropriately during our analysis. We're going to enrich these and create a new geocoded csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e51599",
   "metadata": {},
   "source": [
    "First make sure to install the required packages if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install pandas geopy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6e046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (2.3.2)\n",
      "Requirement already satisfied: geopy in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Users/dejp3/Library/Python/3.9/lib/python/site-packages (from geopy) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332735a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 records from './data/reece1.csv'.\n",
      "Found 22 records missing geocoordinates.\n",
      "Geocoded record 1: Found coordinates for 'Llanfachraeth, Isle of Anglesey, United Kingdom' - Lat: 53.3141694, Lon: -4.5388991\n",
      "Geocoded record 9: Found coordinates for 'Baylham, Suffolk, United Kingdom' - Lat: 52.1282329, Lon: 1.0819174\n",
      "Could not find coordinates for 'Near Bratoft, Lincolnshire, United Kingdom'.\n",
      "Geocoded record 19: Found coordinates for 'Eyke, Suffolk, United Kingdom' - Lat: 52.1166526, Lon: 1.3845169\n",
      "Geocoded record 21: Found coordinates for 'Bedfield, Suffolk, United Kingdom' - Lat: 52.2532916, Lon: 1.2515435\n",
      "Geocoded record 41: Found coordinates for 'Bedfield, Suffolk, United Kingdom' - Lat: 52.2532916, Lon: 1.2515435\n",
      "Geocoded record 42: Found coordinates for 'Great Barton, Suffolk, United Kingdom' - Lat: 52.2709279, Lon: 0.770001\n",
      "Geocoded record 43: Found coordinates for 'North Elmham, Norfolk, United Kingdom' - Lat: 52.7614104, Lon: 0.9262684\n",
      "Geocoded record 49: Found coordinates for 'Ellesmere Rural, Shropshire, United Kingdom' - Lat: 52.9159139, Lon: -2.944987\n",
      "Geocoded record 52: Found coordinates for 'Huxley, Cheshire West and Chester, United Kingdom' - Lat: 53.1482537, Lon: -2.7310092\n",
      "Geocoded record 55: Found coordinates for 'Isle of Wight, Isle of Wight, United Kingdom' - Lat: 50.6710825, Lon: -1.3328043\n",
      "Could not find coordinates for 'Near Drinkstone, Suffolk, United Kingdom'.\n",
      "Geocoded record 58: Found coordinates for 'Uttlesford, Essex, United Kingdom' - Lat: 51.9299895, Lon: 0.2708365\n",
      "Skipping record at index 58: No parish or county information available.\n",
      "Geocoded record 60: Found coordinates for 'Warnford CP, Hampshire, United Kingdom' - Lat: 51.0097254, Lon: -1.1291964\n",
      "Geocoded record 61: Found coordinates for 'Warnford CP, Hampshire, United Kingdom' - Lat: 51.0097254, Lon: -1.1291964\n",
      "Geocoded record 65: Found coordinates for 'Chewton Mendip, Somerset, United Kingdom' - Lat: 51.2738346, Lon: -2.5956\n",
      "Geocoded record 68: Found coordinates for 'Wetheringsett-cum-Brockford, Suffolk, United Kingdom' - Lat: 52.2518975, Lon: 1.1240623\n",
      "Geocoded record 70: Found coordinates for 'Near Basingstoke, Hampshire, United Kingdom' - Lat: 51.2306463, Lon: -1.2269672\n",
      "Geocoded record 72: Found coordinates for 'Warnford CP, Hampshire, United Kingdom' - Lat: 51.0097254, Lon: -1.1291964\n",
      "Could not find coordinates for 'Londesborough area, East Riding of Yorkshire, United Kingdom'.\n",
      "Geocoded record 79: Found coordinates for 'Cavendish, Suffolk, United Kingdom' - Lat: 52.0873118, Lon: 0.6336715\n",
      "\n",
      "Geocoding complete. Successfully geocoded 18 records.\n",
      "Updated data saved to './data/geocoded.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define the input and output filenames\n",
    "input_file = './data/reece1.csv'\n",
    "output_file = './data/geocoded.csv'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: The file '{input_file}' was not found. Please ensure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    # Use a common encoding like 'utf-8' or 'latin1'\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "    print(f\"Loaded {len(df)} records from '{input_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the geocoder with a custom user agent\n",
    "# A user agent is required by many services for proper identification\n",
    "geolocator = Nominatim(user_agent=\"geocoding_script_for_roman_coins\")\n",
    "\n",
    "# Find records that are missing both latitude and longitude\n",
    "missing_coords_df = df[(df['fourFigureLat'].isnull()) | (df['fourFigureLon'].isnull())]\n",
    "print(f\"Found {len(missing_coords_df)} records missing geocoordinates.\")\n",
    "\n",
    "# Check if there are records to geocode\n",
    "if len(missing_coords_df) == 0:\n",
    "    print(\"No missing coordinates to geocode. Saving the original file as 'geocoded.csv'.\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    exit()\n",
    "\n",
    "# Iterate through the records with missing coordinates, using knownas for parish or further details\n",
    "records_geocoded = 0\n",
    "for index, row in missing_coords_df.iterrows():\n",
    "    parish = str(row['knownas']).strip() if pd.notnull(row['knownas']) else ''\n",
    "    county = str(row['county']).strip() if pd.notnull(row['county']) else ''\n",
    "    \n",
    "    # Construct the query string. Adding 'United Kingdom' improves accuracy.\n",
    "    query = f\"{parish}, {county}, United Kingdom\"\n",
    "\n",
    "    if not parish and not county:\n",
    "        print(f\"Skipping record at index {index}: No parish or county information available.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        location = geolocator.geocode(query, timeout=10)\n",
    "        \n",
    "        if location:\n",
    "            # Update the original DataFrame with the new coordinates\n",
    "            df.loc[index, 'fourFigureLat'] = location.latitude\n",
    "            df.loc[index, 'fourFigureLon'] = location.longitude\n",
    "            records_geocoded += 1\n",
    "            print(f\"Geocoded record {index+1}: Found coordinates for '{query}' - Lat: {location.latitude}, Lon: {location.longitude}\")\n",
    "        else:\n",
    "            print(f\"Could not find coordinates for '{query}'.\")\n",
    "\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable) as e:\n",
    "        print(f\"Geocoding service error for '{query}': {e}. Retrying after a short delay.\")\n",
    "        time.sleep(5)  # Pause to avoid rate limiting\n",
    "        location = geolocator.geocode(query) # Try one more time\n",
    "        if location:\n",
    "            df.loc[index, 'fourFigureLat'] = location.latitude\n",
    "            df.loc[index, 'fourFigureLon'] = location.longitude\n",
    "            records_geocoded += 1\n",
    "            print(f\"Geocoded record {index+1}: Found coordinates for '{query}' - Lat: {location.latitude}, Lon: {location.longitude}\")\n",
    "        else:\n",
    "            print(f\"Retry failed. Could not find coordinates for '{query}'.\")\n",
    "\n",
    "    # Be polite and add a short delay between requests to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nGeocoding complete. Successfully geocoded {records_geocoded} records.\")\n",
    "\n",
    "# Save the final, updated DataFrame to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Updated data saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2edc7",
   "metadata": {},
   "source": [
    "So you now have a script that can geocode missing coordinates for your dataset. Now we want to convert this to Linked Pasts geojson format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6ec03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with findIdentifier 'finds-1223903' due to missing coordinates.\n",
      "Skipping row with findIdentifier 'finds-1204869' due to missing coordinates.\n",
      "Skipping row with findIdentifier 'finds-1204740' due to missing coordinates.\n",
      "Skipping row with findIdentifier 'finds-1198637' due to missing coordinates.\n",
      "\n",
      "Successfully converted 76 valid records to ./data/republican.geojson\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def convert_csv_to_geojson(csv_file, geojson_file):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with lat/lon coordinates into a GeoJSON file.\n",
    "    \"\"\"\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"indexing\": {\n",
    "            \"@context\": \"https://schema.org/\",\n",
    "            \"@type\": \"Dataset\",\n",
    "            \"name\": \"Roman Republican Coins from the Portable Antiquities Scheme\",\n",
    "            \"description\": \"An enriched dataset of Roman Republican coins from the Portable Antiquities Scheme\",\n",
    "            \"license\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "            \"identifier\": \"https://finds.org.uk/database/search/results/broadperiod/ROMAN/reeceID/1/\"\n",
    "        },\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                # Create a new, cleaned row dictionary\n",
    "                cleaned_row = {key: value.replace(u'\\xa0', u' ') if isinstance(value, str) else value for key, value in row.items()}\n",
    "                \n",
    "                try:\n",
    "                    # Clean up and validate coordinates\n",
    "                    lat_str = cleaned_row.get('fourFigureLat', '').strip()\n",
    "                    lon_str = cleaned_row.get('fourFigureLon', '').strip()\n",
    "\n",
    "                    # Skip records with empty or invalid coordinates\n",
    "                    if not lat_str or not lon_str:\n",
    "                        print(f\"Skipping row with findIdentifier '{cleaned_row.get('findIdentifier', 'N/A')}' due to missing coordinates.\")\n",
    "                        continue\n",
    "\n",
    "                    lat = float(lat_str)\n",
    "                    lon = float(lon_str)\n",
    "\n",
    "                    # Create a GeoJSON Feature\n",
    "                    # Format 'created' date to YYYY\n",
    "                    created_raw = cleaned_row.get('created')\n",
    "                    created_year = None\n",
    "                    if created_raw:\n",
    "                        try:\n",
    "                            created_year = str(created_raw).strip()[:4]\n",
    "                            if not created_year.isdigit():\n",
    "                                created_year = None\n",
    "                        except Exception:\n",
    "                            created_year = None\n",
    "                    \n",
    "                    feature = {\n",
    "                        \"@id\": f\"https://finds.org.uk/database/artefacts/record/id/{cleaned_row.get('id')}\",\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": {\n",
    "                            \"type\": \"Point\",\n",
    "                            \"coordinates\": [lon, lat]\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"findIdentifier\": cleaned_row.get('findIdentifier'),\n",
    "                            \"oldFindID\": cleaned_row.get('old_findID'),\n",
    "                            \"objecttype\": cleaned_row.get('objecttype'),\n",
    "                            \"broadperiod\": cleaned_row.get('broadperiod'),\n",
    "                            \"description\": cleaned_row.get('description'),\n",
    "                            \"county\": cleaned_row.get('county'),\n",
    "                            \"district\": cleaned_row.get('district'),\n",
    "                            \"parish\": cleaned_row.get('parish'),\n",
    "                            \"knownas\": cleaned_row.get('knownas'),\n",
    "                            \"ruler\": cleaned_row.get('rulerName'),\n",
    "                            \"moneyer\": cleaned_row.get('moneyerName'),\n",
    "                            \"denomination\": cleaned_row.get('denominationName'),\n",
    "                            \"mint\": cleaned_row.get('mintName'),\n",
    "                            \"manufacture\": cleaned_row.get('manufactureTerm'),\n",
    "                            \"rrcType\": cleaned_row.get('rrcType'),\n",
    "                            \"rrcID\": cleaned_row.get('rrcID'),\n",
    "                            \"ricID\": cleaned_row.get('ricID'),\n",
    "                            \"reeceID\": cleaned_row.get('reeceID'),\n",
    "                            \"nomismaIssuer\": cleaned_row.get('rulerNomisma'),\n",
    "                            \"nomismaMint\": cleaned_row.get('nomismaMintID'),\n",
    "                            \"pleiadesID\": cleaned_row.get('pleiadesID'),\n",
    "                            \"issuerDbPedia\": cleaned_row.get('rulerDbpedia'),\n",
    "                            \"metal\": cleaned_row.get('metal'),\n",
    "                            \"materialTerm\": cleaned_row.get('materialTerm'),\n",
    "                            \"weight\": cleaned_row.get('weight'),\n",
    "                            \"date_from\": cleaned_row.get('fromdate'),\n",
    "                            \"date_to\": cleaned_row.get('todate'),\n",
    "                            \"institution\": cleaned_row.get('institution'),\n",
    "                            \"created\": created_year,\n",
    "                        }\n",
    "                    }\n",
    "                    filename = cleaned_row.get('filename', '').strip()\n",
    "                    if filename:\n",
    "                        baseurl = 'https://republican-coins.museologi.st/images/'\n",
    "                        depiction_url = baseurl + filename\n",
    "                        oldfindID = cleaned_row.get('old_findID', '').strip()\n",
    "                        feature['depictions'] = [\n",
    "                            {\n",
    "                                \"@id\": depiction_url,\n",
    "                                \"thumbnail\": depiction_url,\n",
    "                                \"label\": f\"A depiction of {oldfindID}\"\n",
    "                            }\n",
    "                        ]\n",
    "                    description = cleaned_row.get('description', '').strip()\n",
    "                    if description:\n",
    "                        feature['descriptions'] = [\n",
    "                            {\n",
    "                                \"value\": description\n",
    "                            }\n",
    "                        ]\n",
    "                    rrc_id = cleaned_row.get('rrcID', '').strip()\n",
    "                    if rrc_id:\n",
    "                        feature['types'] = [\n",
    "                            {\n",
    "                                \"identifier\": f\"https://numismatics.org/crro/id/{rrc_id.lower()}\",\n",
    "                                \"label\": f\"Nomisma RRC type: {rrc_id.lower()}\"\n",
    "                            }\n",
    "                        ]\n",
    "                    ric_id = cleaned_row.get('ricID', '').strip()\n",
    "                    if ric_id:\n",
    "                        feature['types'] = [\n",
    "                            {\n",
    "                                \"identifier\": f\"https://numismatics.org/ocre/id/{ric_id.lower()}\",\n",
    "                                \"label\": f\"Nomisma RIC type: {ric_id.lower()}\"\n",
    "                            }\n",
    "                        ]\n",
    "                    # Add 'when' key only if 'fromDate' is present\n",
    "                    from_date = cleaned_row.get('fromdate', '').strip().split('.')[0]\n",
    "                    if from_date:\n",
    "                        to_date = cleaned_row.get('todate', '').strip().split('.')[0]\n",
    "                        if to_date:\n",
    "                            feature['when'] = {\n",
    "                                \"timespans\": [\n",
    "                                    {\n",
    "                                        \"start\": {\n",
    "                                            \"in\": f\"{from_date}\" if from_date else \"\"\n",
    "                                        },\n",
    "                                        \"end\": {\n",
    "                                            \"in\": f\"{to_date}\" if to_date else \"\",\n",
    "                                        }\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"periods\": [\n",
    "                                    {\n",
    "                                        \"name\": \"Roman Republican 510 BC - 27 BC\",\n",
    "                                        \"uri\": \"http://n2t.net/ark:/99152/p08m57h65c8\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"label\": \"for a century during the Roman period\",\n",
    "                                \"certainty\": \"certain\",\n",
    "                                \"duration\": \"P100Y\"\n",
    "                            }\n",
    "\n",
    "                    links = []\n",
    "                    pleiades_id = cleaned_row.get('pleiadesID', '').strip()\n",
    "                    if pleiades_id and pleiades_id.replace('.', '', 1).isdigit():\n",
    "                        pleiades_id = str(int(float(pleiades_id)))\n",
    "                    nomisma_mint_id = cleaned_row.get('nomismaMintID', '').strip()\n",
    "                    moneyer_id = cleaned_row.get('moneyerID', '').strip()\n",
    "                    dbpedia_issuer = cleaned_row.get('rulerDbpedia', '').strip()\n",
    "                    nomisma_issuer = cleaned_row.get('rulerNomisma', '').strip()\n",
    "                    nomisma_reece_id = cleaned_row.get('reeceID', '').strip()\n",
    "\n",
    "                    if nomisma_issuer:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://nomisma.org/id/{nomisma_issuer}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": f\"Nomisma ruler {nomisma_issuer}\"\n",
    "                        })\n",
    "                    if pleiades_id:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://pleiades.stoa.org/places/{pleiades_id}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": f\"Pleiades place {pleiades_id}\"\n",
    "                        })\n",
    "                    if nomisma_mint_id:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://nomisma.org/id/{nomisma_mint_id}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": f\"Nomisma mint {nomisma_mint_id}\"\n",
    "                        })\n",
    "                    if nomisma_reece_id:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://nomisma.org/id/reece{nomisma_reece_id}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": \"Nomisma Reece Period 1\"\n",
    "                        })\n",
    "                    if moneyer_id:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://nomisma.org/id/{moneyer_id}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": f\"Nomisma moneyer {moneyer_id}\"\n",
    "                        })\n",
    "                    if dbpedia_issuer:\n",
    "                        links.append({\n",
    "                            \"identifier\": f\"https://dbpedia.org/resource/{dbpedia_issuer}\",\n",
    "                            \"type\": \"seeAlso\",\n",
    "                            \"label\": f\"DBpedia resource for {dbpedia_issuer}\"\n",
    "                        })\n",
    "                    if links:\n",
    "                        feature['links'] = links\n",
    "\n",
    "                    geojson['features'].append(feature)\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"Skipping row with findIdentifier '{cleaned_row.get('findIdentifier', 'N/A')}' due to invalid coordinate data: {e}\")\n",
    "                    continue\n",
    "\n",
    "        with open(geojson_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"\\nSuccessfully converted {len(geojson['features'])} valid records to {geojson_file}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"./data/geocoded.csv\"\n",
    "    output_geojson = \"./data/republican.geojson\"\n",
    "    convert_csv_to_geojson(input_csv, output_geojson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69d053",
   "metadata": {},
   "source": [
    "And there you go, that's how you create Linked Pasts geojson files from your CSV data ready to use in a Peripleo instance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
